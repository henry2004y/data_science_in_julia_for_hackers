[["basketball-shots.html", "Chapter 8 Basketball shots 8.1 Modelling the scoring probability 8.2 Prior Predictive Checks: Part I 8.3 New model and prior predictive checks: Part II 8.4 Does the Period affect the probability of scoring? 8.5 Summary", " Chapter 8 Basketball shots When playing basketball we can ask ourselves: how likely is it to score given a position in the court? To answer this question we are going to use data from NBA games from the 2006 - 2007 season. Lets load the data into our Julia session and inspect the column names: shots = CSV.read(&quot;./08_basketball_shots/data/seasons/shots_season_2006_2007.csv&quot;, DataFrame); names(shots) ## 5-element Vector{String}: ## &quot;result&quot; ## &quot;x&quot; ## &quot;y&quot; ## &quot;period&quot; ## &quot;time&quot; The columns of our dataset are the following: result: Stores a 1 if the shot was successful and a 0 otherwise. x: The x-component of the position of the player when the shot was made. y: The y-component of the position of the player when the shot was made. period: Indicates in which of the four periods of a basketball game the shot was done. time: The exact time the shot was made. Below we show a sketch of a basketball court, its dimensions and how to interpret the data in the table. It would be useful to change the shot coordinates from cartesian (x and y coordinates) to polar. In this way, we can think about the problem considering the distance and the angle from the hoop. shots[!, :distance] = sqrt.( shots.x .^ 2 + shots.y .^ 2) shots[!, :angle] = atan.( shots.y ./ shots.x ) filter!(x -&gt; x.distance &gt; 1, shots) So, the \\(x\\) and \\(y\\) axis have their origin at the hoop, and we compute the distance from this point to where the shot was made. Also, we compute the angle with respect to the \\(x\\) axis, showed as θ in the sketch. Lets now plot where the shots were made with a two-dimensional histogram. As more shots are made in a certain region of the field, that region is shown with a brighter color. histogram2d(shots.y[1:10000], shots.x[1:10000], bins=(50,30)); xlabel!(&quot;x axis&quot;); ylabel!(&quot;y axis&quot;) We see that the shots are very uniformly distributed around the hoop, except for distances very near to the hoop. To see this better, we plot the histograms for each axis, \\(x\\) and \\(y\\). As we are interested in the shots that were scored, we filter the shots made and plot the histogram of each axis. shots_made = filter(x -&gt; x.result == 1, shots) histogram(shots_made.y[1:10000], legend=false, nbins=40); xlabel!(&quot;x axis&quot;); ylabel!(&quot;Counts&quot;) histogram(shots_made.x[1:10000], legend=false, nbins=45); xlabel!(&quot;y axis&quot;); ylabel!(&quot;Counts&quot;) We can also summarize all this information with a wireplot, as shown below h = fit(Histogram, (shots_made.y, shots_made.x), nbins=40); wireframe(midpoints(h.edges[2]), midpoints(h.edges[1]), h.weights, zlabel=&quot;counts&quot;, xlabel=&quot;y&quot;, ylabel=&quot;x&quot;, camera=(40,40)); xlabel!(&quot;x&quot;); ylabel!(&quot;y&quot;); title!(&quot;Histogram of shots scored&quot;) More shots are made as we get near the hoop, as expected. It is worth noting that we are not showing the probability of scoring, we are just showing the distribution of shot scored, not how likely is it to score. 8.1 Modelling the scoring probability The first model we are going to propose is a Bernoulli model. A Bernoulli distribution results from an experiment in which we have two possible outcomes, one that is usually called a success and another called a failure. In our case our success is scoring the shot and the other possible event is failing it. The only parameter needed in a bernoulli distribution is the probability \\(p\\) of having a success. We are going to model this parameter as a logistic function: plot(logistic, legend=false); xlabel!(&quot;x&quot;); ylabel!(&quot;Probability&quot;); title!(&quot;Logistic function (x)&quot;) The reason to choose a logistic function is that we are going to model the probability of shootiing as a function of some variables, for example the distance to the hoop, and we want that our scoring probability increases as we get closer to it. Also out probability needs to be between 0 an 1, so a nice function to map our values is the logistic function. The probabilistic model we are going to propose is \\(p\\sim logistic(a + b*distance[i] + c*angle[i])\\) \\(outcome[i]\\sim Bernoulli(p)\\) As you can see, this is a very general model, since we haven’t yet specified anything about \\(a\\), \\(b\\) and \\(c\\). Our approach will be to propose prior distributions for each one of them and check if our proposals make sense. 8.2 Prior Predictive Checks: Part I Lets start by proposing Normal prior distributions for \\(a\\), \\(b\\) and \\(c\\), i.e., Gaussian distributions with mean 0 and variance 1. Lets sample and see what are the possible predictions for \\(p\\): \\(a\\sim N(0,1)\\) \\(b\\sim N(0,1)\\) \\(c\\sim N(0,1)\\) possible_distances = 0:0.01:1 possible_angles = 0:0.01:π/2 n_samples = 100 # we sample possible values from a and b a_prior_sampling = rand(Normal(0,1), n_samples) b_prior_sampling = rand(Normal(0,1), n_samples) # with the sampled values of a and b, we make predictions about how p would look predicted_p = [] for i in 1:n_samples push!(predicted_p, logistic.(a_prior_sampling[i] .+ b_prior_sampling[i] .* possible_distances)) end plot(possible_distances, predicted_p[1], legend = false, color=&quot;blue&quot;); for i in 2:n_samples plot!(possible_distances, predicted_p[i], color=:blue); end xlabel!(&quot;Normalized distance&quot;); ylabel!(&quot;Predicted probability&quot;); title!(&quot;Prior predictive values for p&quot;) Each one of these plots is the result of the logistic with one combination of \\(a\\) and \\(b\\) as parameters. We see that some of the predicted values of \\(p\\) don’t make sense. For example, if \\(b\\) takes positive values, we are saying that as we increase our distance from the hoop, the probability of scoring also increases. As we want \\(b\\) to be a negative number, we should propose a distribution which we can sample from and be sure their values have always the same sign. One example of this is the LogNormal distribution, which will give us always positive numbers. Multiplying these values by \\(-1\\) gives us the certainty that we will have a negative number. We update our model as follows: \\(a\\sim Normal(0,1)\\) \\(b\\sim LogNormal(1,0.25)\\) \\(c\\sim Normal(0,1)\\) Repeating the sampling process with the updated model, we get the following predictions for \\(p\\): b_prior_sampling_negative = rand(LogNormal(1,0.25), n_samples) predicted_p_inproved = [] for i in 1:n_samples push!(predicted_p_inproved, logistic.(a_prior_sampling[i] .- b_prior_sampling_negative[i].*possible_distances)) end plot(possible_distances, predicted_p_inproved[1], legend = false, color=:blue); for i in 2:n_samples plot!(possible_distances, predicted_p_inproved[i], color=:blue); end xlabel!(&quot;Normalized distance&quot;); ylabel!(&quot;Predicted probability&quot;); title!(&quot;Prior predictive values for p with negative LogNormal prior&quot;) Now the behavior we can see from the predicted \\(p\\) curves is the expected. As the shooting distance increases, the probability of scoring decreases. We have set some boundaries in the form of a different prior probability distribution. This process is what is called prior-predictive checks. Esentially, it is an iterative process where we check if our initial proposals make sense. Now that we have the expected behaviour for \\(p\\), we define our model and calculate the posterior distributions with our data points. 8.2.1 Defining our model in Turing and computing posteriors Now we define our model in the Turing framework in order to sample from it: @model logistic_regression(distances, angles, result,n) = begin N = length(distances) # Set priors. a ~ Normal(0,1) b ~ LogNormal(1,0.25) c ~ Normal(0,1) for i in 1:n p = logistic(a - b*distances[i] + c*angles[i]) result[i] ~ Bernoulli(p) end end n = 1000 The output of the sampling tells us also some information about sampled values for our parameters, like the mean, the standard deviation and some other computations. # Sample using HMC. chain = mapreduce(c -&gt; sample(logistic_regression(shots.distance[1:n] ./ maximum(shots.distance[1:n] ), shots.angle[1:n], shots.result[1:n], n), NUTS(), 1500), chainscat, 1:3); 8.2.1.1 Traceplot In the plot below we show a traceplot of the sampling. When we run a model and calculate the posterior, we obtain sampled values from the posterior distributions. We can tell our sampler how many sampled values we want. A traceplot just shows them in sequential order. We also can plot the distribution of those values, and this is what is showed next to each traceplot. plot(chain, dpi=60) a_mean = mean(chain[:a]) b_mean = mean(chain[:b]) c_mean = mean(chain[:c]) Now plotting the scoring probability using the posterior distributions of \\(a\\), \\(b\\) and \\(c\\) for an angle of 45°, we get: p_constant_angle = [] for i in 1:length(chain[:a]) push!(p_constant_angle, logistic.(chain[:a][i] .- chain[:b][i].*possible_distances .+ chain[:c][i].*π/4)); end plot(possible_distances,p_constant_angle[1], legend=false, alpha=0.1, color=:blue); for i in 2:1000 plot!(possible_distances,p_constant_angle[i], alpha=0.1, color=:blue); end xlabel!(&quot;Normalized distance&quot;); ylabel!(&quot;Probability&quot;); title!(&quot;Scoring probability vs Normalized distance (angle=45°)&quot;) The plot shows that the probability of scoring is higher as our distance to the hoop decrease, which makes sense, since the difficulty of scoring increase. We plot now how the probability varies with the angle for a given distance. Here we plot for a mid distance, corresponding to 0.5 in a normalized distance. p_constant_distance = [] for i in 1:length(chain[:a]) push!(p_constant_distance, logistic.(chain[:a][i] .- chain[:b][i].*0.5 .+ chain[:c][i].*possible_angles)); end plot(rad2deg.(possible_angles),p_constant_distance[1], legend=false, alpha=0.1, color=:blue); for i in 2:1000 plot!(rad2deg.(possible_angles),p_constant_distance[i], alpha=0.1, color=:blue); end xlabel!(&quot;Angle [deg]&quot;); ylabel!(&quot;Probability&quot;); title!(&quot;Scoring probability vs Angle (mid distance)&quot;) We see that the model predicts almost constant probability as we vary the angle. This makes sense, considering that the angle doesn’t seem too relevant when shooting; the distance from the hoop is the relevant variable. 8.3 New model and prior predictive checks: Part II Now we propose another model with the form: \\(p\\sim logistic(a+ b^{distance[i]} + c*angle[i])\\) *But for what values of b the model makes sense? We show below the plot for 4 function with 4 possible values of \\(b\\), having in mind that the values of \\(x\\), the normalized distance, goes from 0 to 1. f1(x) = 0.3^x f2(x) = 1.5^x f3(x) = -0.3^x f4(x) = -1.5^x plot(0:0.01:1, f1, label=&quot;f1: b &lt; 1 &amp; b &gt; 0&quot;, xlim=(0,1), ylim=(-2,2), lw=3); plot!(0:0.01:1, f2, label=&quot;f2: b&gt;1&quot;, lw=3); plot!(0:0.01:1, f3, label=&quot;f3: b&lt;0 &amp; b&gt;-1&quot;, lw=3); plot!(0:0.01:1, f4, label=&quot;f3: b&lt;-1&quot;, lw=3); xlabel!(&quot;Normalized distance&quot;); title!(&quot;Prior Predictive influence of distance&quot;) Analysing the possible values for \\(b\\), the one that makes sense is the value proposed in f1, since we want an increasing influence of the distance in the values of \\(p\\) as the distance decreases, since the logistic function has higher values for higher values of x. So now that we know the values the our parameter \\(b\\) can take, we propose for it a beta distribution with parameters α=2 and β=5, showed in the plot below. plot(Beta(2,5), xlim=(-0.1,1), legend=false); title!(&quot;Prior distribution for b&quot;) 8.3.1 Defining the new model and computing posteriors We define then our model and calculate the posterior as before. @model logistic_regression_exp(distances, angles, result, n) = begin N = length(distances) # Set priors. a ~ Normal(0,1) b ~ Beta(2,5) c ~ Normal(0,1) for i in 1:n p = logistic(a + b .^ distances[i] + c*angles[i]) result[i] ~ Bernoulli(p) end end ## logistic_regression_exp (generic function with 2 methods) # Sample using HMC. chain_exp = mapreduce(c -&gt; sample(logistic_regression_exp(shots.distance[1:n] ./ maximum(shots.distance[1:n] ), shots.angle[1:n], shots.result[1:n], n), HMC(0.05, 10), 1500), chainscat, 1:3) Plotting the traceplot we see again that the variable angle has little importance since the parameter \\(c\\), that can be related to the importance of the angle variable for the probability of scoring, is centered at 0. plot(chain_exp, dpi=55) p_exp_constant_angle = [] for i in 1:length(chain_exp[:a]) push!(p_exp_constant_angle, logistic.(chain_exp[:a][i] .+ chain_exp[:b][i].^possible_distances .+ chain_exp[:c][i].*π/4)) end Employing the posteriors distributions computed, we plot the probability of scoring as function of the normalized distance and obtain the plot shown below. plot(possible_distances,p_exp_constant_angle[1], legend=false, alpha=0.1, color=:blue); for i in 2:1000 plot!(possible_distances,p_exp_constant_angle[i], alpha=0.1, color=:blue); end xlabel!(&quot;Normalized distance&quot;); ylabel!(&quot;Probability&quot;); title!(&quot;Scoring probability vs Normalized distance (angle=45°)&quot;) Given that we have 2 variables, we can plot the mean probability of scoring as function of the two and obtain a surface plot. We show this below. angle_ = collect(range(0, stop=π/2, length=100)) dist_ = collect(range(0, stop=1, length=100)) it = Iterators.product(angle_, dist_) matrix = collect.(it) values = reshape(matrix, (10000, 1)) angle_grid = getindex.(values,[1]) dist_grid = getindex.(values,[2]) z = logistic.(mean(chain_exp[:a]) .+ mean(chain_exp[:b]).^dist_grid .+ mean(chain_exp[:c]).*angle_grid) The plot show the behaviour expected, an increasing probability of scoring as we get near the hoop. We also see that there is almost no variation of the probability with the angle. 8.4 Does the Period affect the probability of scoring? Now we will try to answer this question. We propose then a model, and calculate the posterior for its parameters with data of one of each of the four possible periods. We define the same model for all four periods. Also, we don’t take into account now the angle variable, since we have seen before that this variable is of little importance. We filter then our data by its period and proceed to estimate our posterior distributions. shots_period1= filter(x -&gt; x.period == 1, shots) @model logistic_regression_period(distances, result,n) = begin N = length(distances) # Set priors. a ~ Normal(0,1) b ~ Beta(2,5) for i in 1:n p = logistic( a + b .^ distances[i]) result[i] ~ Bernoulli(p) end end n_ = 500 # Sample using HMC. chain_period1 = mapreduce(c -&gt; sample(logistic_regression_period(shots_period1.distance[1:n_] ./ maximum(shots_period1.distance[1:n_] ),shots_period1.result[1:n_], n_), HMC(0.05, 10), 1500), chainscat, 1:3) shots_period2 = filter(x -&gt; x.period == 2, shots) # Sample using HMC. chain_period2 = mapreduce(c -&gt; sample(logistic_regression_period(shots_period2.distance[1:n_] ./ maximum(shots_period2.distance[1:n_] ), shots_period2.result[1:n_], n_), HMC(0.05, 10), 1500), chainscat, 1:3 ); shots_period3= filter(x-&gt;x.period==3, shots); # Sample using HMC. chain_period3 = mapreduce(c -&gt; sample(logistic_regression_period(shots_period3.distance[1:n_] ./ maximum(shots_period3.distance[1:n_] ), shots_period3.result[1:n_], n_), HMC(0.05, 10), 1500), chainscat, 1:3 ); shots_period4 = filter(x-&gt;x.period==4, shots); # Sample using HMC. chain_period4 = mapreduce(c -&gt; sample(logistic_regression_period(shots_period4.distance[1:n_] ./ maximum(shots_period4.distance[1:n_]), shots_period4.result[1:n_], n_), HMC(0.05, 10), 1500), chainscat, 1:3 ); p_period1 = logistic.(mean(chain_period1[:a]) .+ mean(chain_period1[:b]).^possible_distances ) p_period1_std = logistic.((mean(chain_period1[:a]).+std(chain_period1[:a])) .+ (mean(chain_period1[:b]).+std(chain_period1[:a])).^possible_distances) p_period2 = logistic.(mean(chain_period2[:a]) .+ mean(chain_period2[:b]).^possible_distances ) p_period2_std = logistic.((mean(chain_period2[:a]).+std(chain_period2[:a])) .+ (mean(chain_period2[:b]).+std(chain_period2[:a])).^possible_distances) p_period3 = logistic.(mean(chain_period3[:a]) .+ mean(chain_period3[:b]).^possible_distances) p_period3_std = logistic.((mean(chain_period3[:a]).+std(chain_period3[:a])) .+ (mean(chain_period3[:b]).+std(chain_period3[:a])).^possible_distances) p_period4 = logistic.(mean(chain_period4[:a]) .+ mean(chain_period4[:b]).^possible_distances ) p_period4_std = logistic.((mean(chain_period4[:a]).+std(chain_period4[:a])) .+ (mean(chain_period4[:b]).+std(chain_period4[:a])).^possible_distances ) We plot now for each period the probability of scoring for each period, each mean and one standard deviation from it. plot(possible_distances, p_period4,ribbon=p_period4_std.-p_period4, color=:magenta, label=&quot;period4&quot;, fillalpha=.3, ylim=(0,0.6)); plot!(possible_distances, p_period2, color=:green, ribbon=p_period2_std.-p_period2, label=&quot;period2&quot;, fillalpha=.3); plot!(possible_distances, p_period3, color=:orange, ribbon=p_period3_std.-p_period3, label=&quot;period3&quot;,fillalpha=.3); plot!(possible_distances, p_period1,ribbon=p_period1_std.-p_period1, color=:blue, label=&quot;period1&quot;, fillalpha=.3); xlabel!(&quot;Normalized distance&quot;); ylabel!(&quot;Scoring probability&quot;) Finally, we see that for the periods 1 and 4, the first and the last periods, the probabity of scoring is slightly higher than the other two periods, meaning that players are somewhat better scoring in those periods. 8.5 Summary In this chapter, we used the NBA shooting data of the season 2006-2007 to analyze how the scoring probability is affected by some variables, such as the distance from the hoop and the angle of shooting. First, we inspected the data by plotting a heatplot of all the shots made and making histograms of the ones that scored. As our goal was to study the probability of scoring, which is a Bernoulli trial situation, we decided to use a Bernoulli model. Since the only parameter needed in a Bernoulli distribution is the probability \\(p\\) of having a success, we modeled \\(p\\) as a logistic function: \\(p\\sim logistic(a+ b*distance[i] + c*angle[i])\\) We set the prior probability of the parameters \\(a\\) and \\(c\\) to a normal distribution and \\(b\\) to a log-normal one. Thus, we constructed our logistic regression model and sampled it using the Markov Monte Carlo algorithm. To gain a better understanding of the sampling process, we made a traceplot that shows the sampled values in a sequential order. Later, we decided to try with a more complex logistic regression model, similar to the first one but this time modifying the distance parameter: \\(p\\sim logistic(a+ b^{distance[i]} + c*angle[i])\\) We set the prior distribution of \\(b\\) to a beta distribution and constructed the second logistic regression model, sampled it and plotted the results. Finally, we analyzed the results to see if the period of the game affects the probability of scoring. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
